<!DOCTYPE html>
<html>

<head>
    
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="chrome=1">
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="referrer" content="no-referrer">
<meta name="description" content="My personal blog">
<title>
Diving into NLP with Topic Modelling - 
</title>




<link rel="shortcut icon" href="/sam.ico">








<link rel="stylesheet" href="/css/main.min.81bbafc4df93b11c1c3e2449464373c384aa4903731b4fc7a77dfcdd979e184f.css" integrity="sha256-gbuvxN&#43;TsRwcPiRJRkNzw4SqSQNzG0/Hp3383ZeeGE8=" crossorigin="anonymous" media="screen">



 

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Didact+Gothic">

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://chongniki.github.io/posts/images/twitteractivity.jpg"/>

<meta name="twitter:title" content="Diving into NLP with Topic Modelling"/>
<meta name="twitter:description" content="Over the summer of 2017, I was fortunate enough to land a Data Science internship at the Center of Applied Data Science (Malaysia). My colleagues were incredibly patient and supportive throughout my time there.
During my time there, I was introduced to Natural Language Processing, where textual data could be machine readable, and understandable. This was mind-blowing! Language, one of the attributes that make human, human - could be understood by machines; This thought astounded me."/>

<meta property="og:title" content="Diving into NLP with Topic Modelling" />
<meta property="og:description" content="Over the summer of 2017, I was fortunate enough to land a Data Science internship at the Center of Applied Data Science (Malaysia). My colleagues were incredibly patient and supportive throughout my time there.
During my time there, I was introduced to Natural Language Processing, where textual data could be machine readable, and understandable. This was mind-blowing! Language, one of the attributes that make human, human - could be understood by machines; This thought astounded me." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://chongniki.github.io/posts/topic_modelling_project/" />

<meta property="og:image" content="https://chongniki.github.io/posts/images/twitteractivity.jpg" />

<meta property="og:image" content="https://chongniki.github.io/posts/images/twitteroverhour.jpeg" />

<meta property="og:image" content="https://chongniki.github.io/posts/images/topicmodel.jpeg" />
<meta property="article:published_time" content="2019-09-09T16:32:25-07:00" />
<meta property="article:modified_time" content="2019-09-09T16:32:25-07:00" /><meta property="og:site_name" content="I&#39;m Niki" />


    

    
    
    
    <title>
        
        Diving into NLP with Topic Modelling
        
    </title>
</head>

<body>
    <div class="wrap">
        <div class="section" id="title">Diving into NLP with Topic Modelling</div>

        
<div class="section" id="content">
    Mon Sep 09, 2019 &#183; 693 words
    
    <hr/>
    <p>Over the summer of 2017, I was fortunate enough to land a Data Science internship at the Center of Applied Data Science (Malaysia). My colleagues were incredibly patient and supportive throughout my time there.</p>

<p>During my time there, I was introduced to Natural Language Processing, where textual data could be machine readable, and understandable. This was mind-blowing! Language, one of the attributes that make human, human - could be understood by machines; This thought astounded me.</p>

<p>Which brings me to the topic of Topic Modelling, a branch in Natural Language Processing where  instances of text are treated as documents and each document are assumed to have a mixture of topics. This method of Natural Language processing teases out the hidden topics from documents using the probability of recurring words within a topic.</p>

<p>So like any Data-driven project, we need data. What better source than Twitter? Filled with Topics, overflowing with Text. Initially, my plan was to get a Twitter API and train my Topic Model on tweets that are from individuals of interest. However my Twitter Developer application was only approved after a year. So I decided to train the model on MY own tweets instead. Mind you, I was only active on Twitter whilst I was in High School, so its safe to say that my tweets were nothing short of <em>embarassing</em>.</p>

<p>Cleaning the data was not easy. Because tweets would normally contain links, symbols, and/or emojis, this took quite a while.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">strip_links</span>(text):
    link_regex    <span style="color:#f92672">=</span> re<span style="color:#f92672">.</span>compile(<span style="color:#e6db74">&#39;((https?):((//)|(</span><span style="color:#ae81ff">\\\\</span><span style="color:#e6db74">))+([\w\d:#@%/;$()~_?\+-=</span><span style="color:#ae81ff">\\</span><span style="color:#e6db74">\.&amp;](#!)?)*)&#39;</span>, re<span style="color:#f92672">.</span>DOTALL)
    links         <span style="color:#f92672">=</span> re<span style="color:#f92672">.</span>findall(link_regex, text)
    <span style="color:#66d9ef">for</span> link <span style="color:#f92672">in</span> links:
        text <span style="color:#f92672">=</span> text<span style="color:#f92672">.</span>replace(link[<span style="color:#ae81ff">0</span>], <span style="color:#e6db74">&#39;, &#39;</span>)
    
    <span style="color:#66d9ef">return</span> text</code></pre></div>
<p>Above is an example function that I used to remove all the links with the use of regex.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">strip_emojis</span>(text):
    <span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">    Remove emojis and other symbols
</span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
    
    emoji <span style="color:#f92672">=</span> re<span style="color:#f92672">.</span>compile(<span style="color:#e6db74">&#34;[&#34;</span>
        <span style="color:#e6db74">u</span><span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\U0001F600</span><span style="color:#e6db74">-</span><span style="color:#ae81ff">\U0001F64F</span><span style="color:#e6db74">&#34;</span>  <span style="color:#75715e"># emoticons</span>
        <span style="color:#e6db74">u</span><span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\U0001F300</span><span style="color:#e6db74">-</span><span style="color:#ae81ff">\U0001F5FF</span><span style="color:#e6db74">&#34;</span>  <span style="color:#75715e"># symbols &amp; pictographs</span>
        <span style="color:#e6db74">u</span><span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\U0001F680</span><span style="color:#e6db74">-</span><span style="color:#ae81ff">\U0001F6FF</span><span style="color:#e6db74">&#34;</span>  <span style="color:#75715e"># transport &amp; map symbols</span>
        <span style="color:#e6db74">u</span><span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\U0001F1E0</span><span style="color:#e6db74">-</span><span style="color:#ae81ff">\U0001F1FF</span><span style="color:#e6db74">&#34;</span>  <span style="color:#75715e"># flags (iOS)</span>
                           <span style="color:#e6db74">&#34;]+&#34;</span>, flags<span style="color:#f92672">=</span>re<span style="color:#f92672">.</span>UNICODE)
    text <span style="color:#f92672">=</span> emoji<span style="color:#f92672">.</span>sub(<span style="color:#e6db74">r</span><span style="color:#e6db74">&#39;&#39;</span>, text)
    
    <span style="color:#66d9ef">return</span> text</code></pre></div>
<p>I also had to remove all the emojis from my tweets - as a 13 year-old girl discovering an outlet to freely express herself, I had alot. There were other functions that removed numbers, stop words, and symbols, which can all be found on my github <a href="https://github.com/chongniki/twitter-topic-modelling">repo</a>.</p>

<p>After text cleaning, I applied NLTK&rsquo;s wordnet lemmatizer to convert the word back to its &ldquo;original&rdquo; form. So for example the word &ldquo;playing&rdquo; would turn to &ldquo;play&rdquo;, and &ldquo;singing&rdquo; would turn to &ldquo;sing&rdquo;. This should not be confused by its counter part - Stemming. Which basically takes a word and chops the end of it. So &ldquo;confused&rdquo; would just become &ldquo;confus&rdquo;.</p>

<p>To understand the data better, I plotted my Twitter activity over time to predict the kind of topics we&rsquo;d be getting.</p>

<p><img src="/images/twitteractivity.jpeg" alt="Twitter activity over the years" /></p>

<p>My activity was the highest September of 2010, and gradually decreased till now. As expected, most of the topics are going to be terrifyingly embarrassing.</p>

<p><img src="/images/twitteroverhour.jpeg" alt="Tweet frequency over hours" /></p>

<p>I also found that 14% of my tweets were tweeted 10pm at night. These plots were created with <a href="https://plot.ly/">plotly</a>, a really great open source library for creating interactive graphs.</p>

<p>As for Topic Modelling, I created a dictionary of my data and converted it into bag-of-words. I then applied the term frequency - inverse document frequency (TF-IDF) to evaluate the importance of each word in a topic.</p>

<p><img src="/images/topicmodel.jpeg" alt="Topic Model" /></p>

<p>When the model is generated, it should come out looking like this. <a href="https://pyldavis.readthedocs.io/en/latest/modules/API.html">pyLDAvis</a> is a great python package for the visualisation of topic models. The blue bars represent the overall frequency of a word in the entire corpus (collection of documents), while the red bars represent the frequency of a word in the topic. The circles are the identified topics, the largest circle being the most frequent topic to appear in the entire corpus. In this case, topic 2, which appears in 22% of the entire corpus is quite revolved around school, and exams.</p>

<p>In conclusion, this project was a great opportunity for me to get a taste of preprocessing, manipulating, and applying models to data. It has opened my eyes to the world of Data, and its boundless application opportunities. I&rsquo;ve only scratched the tip of the iceberg over the course of this project, and am eager to learn more. Happy coding!</p>

</div>


        
<div class="section bottom-menu">
    
<hr />
<p>


    
        <a href="/posts">back</a>
        
            &#183;
        
    

    
        
            <a href="/projects">
                projects
            </a>
        
    
    
        
            &#183; 
            <a href="/links">
                links
            </a>
        
            &#183; 
            <a href="/about">
                Hi, I&#39;m Niki.
            </a>
        
    
    &#183; 
    <a href="https://chongniki.github.io/">
        home
    </a>

</p>
</div>


        <div class="section footer">Hello! What inspires you today?</div>
    </div>
</body>

</html>